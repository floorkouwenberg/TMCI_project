{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ProductId  Score                                               Text\n",
      "7812  B00474H936      4  I have tried almost every kind of \"fake meat\" ...\n",
      "2304  B0002Z9BF8      5  <a href=\"http://www.amazon.com/gp/product/B000...\n",
      "5224  B0009OMU00      5  I tasted this snack at my local pub and found ...\n",
      "7062  B004K30HO2      1  I was so excited to get these k cups and immed...\n",
      "7105  B004K30HO2      4  I had been looking for something to use with m...\n",
      "3342  B005K4Q1VI      5  This is the absolute best hot cocoa for the Ke...\n",
      "8473  B003VXFK44      5  It was a bit of a risk to order this, I mean w...\n",
      "5999  B003SE52K8      5  We tried all flavors for our cats and the rabb...\n",
      "7748  B001E5DZTS      5  This is a really good source for goats milk. I...\n",
      "907   B000ER6YO0      4  I like the Earth's Best baby food line because...\n",
      "5735  B003TIVBSU      5  I don't understand(believe) the ONE negative r...\n",
      "7591  B000CQ6KSI      5  No long, frightening lists of chemical additiv...\n",
      "7500  B000S11CD0      5  Cant make corn tortillas or tamales without ma...\n",
      "127   B003OB0IB8      4  I've been a fan of Hot & Spicy Maruchan Ramen ...\n",
      "5118  B003QM6ZDU      5  I bought this on a whim and wow do I consider ...\n",
      "2038  B001E5E29A      4  The entire family loved this pancake mix. It w...\n",
      "4275  B004391DK0      5  I was so excited when Bisquick came out with t...\n",
      "7524  B000OIWY8Y      5  This tea is my favorite, I've never had any te...\n",
      "9523  B000N648QI      5  This food looks just like something you would ...\n",
      "933   B000ER6YO0      5  This tastes so good that apparently our cat th...\n",
      "5274  B001AQS7JE      4  Great candy! Sweet and intense, low calorie an...\n",
      "1344  B000ODRY9I      5  Like the flavor and it's thick, unlike some fa...\n",
      "6612  B002BRC562      4  When I opened the package, I only found 11 pac...\n",
      "3811  B000X2CWTM      5  This is the best licorice in the USA. If you h...\n",
      "7120  B004K30HO2      5  Works great. So much easier to use than the on...\n",
      "482   B000G6RYNE      5  These chips are quite tasty and the price is r...\n",
      "185   B001KUUNP6      5  We had trouble finding this locally - delivery...\n",
      "3582  B004MDQSDO      5  I rarely eat anything but whole wheat pasta, b...\n",
      "8740  B000LKZ84C      5  This is the greatest stuff on the planet. Firs...\n",
      "315   B001EPQ0J0      5  This brand of coffee has been available in my ...\n",
      "...          ...    ...                                                ...\n",
      "1460  B0040WHJQM      5  <span class=\"tiny\"> Length:: 1:38 Mins<br /><b...\n",
      "5910  B001DIM8K8      5  I'll be honest, if you want a quick breakfast ...\n",
      "1131  B003UDSXU8      5  I am very acustom to spicy foods and this has ...\n",
      "4906  B0009JMW1C      5  I'm very happy with this-- drains my ears and ...\n",
      "2993  B0009JI7O8      5  I brought these cookies as a \"dish to pass\" at...\n",
      "7858  B000ELWDRI      5  Very tasty but my only complaint is that it's ...\n",
      "6281  B000KOSDY6      2  so so.there was great reviews on product only ...\n",
      "4247  B004391DK0      5  This makes the best waffles that I have had in...\n",
      "5873  B008AHJZTM      1  This was not good. The flavor was lacking and ...\n",
      "8880  B001EQ5EQE      2  I usually order Bustelo Decaffeinated coffee a...\n",
      "5259  B0012YEKCW      5  These seeds always grow, unlike some other bra...\n",
      "7118  B004K30HO2      5  I purchased these on a whim, and after researc...\n",
      "8488  B003VXFK44      5  I LOVED THIS COFFEE. I just got my order a few...\n",
      "4784  B00139TT72      5  I switched my mini-Schauzers over to Newman's ...\n",
      "4959  B002RZ1QNG      5  This is my third wine kit and it's going so fa...\n",
      "8417  B003VXFK44      5  This is the smoothest and best tasting Keurig ...\n",
      "5176  B003FWKCB2      5  My four kids LOVE these, it's great that there...\n",
      "4043  B000CMHMUC      4  I love this flavor of Soy Crisps...Deep Sea Sa...\n",
      "6617  B001LG945O      4  I have to agree with several other reviews her...\n",
      "7497  B0000T15GY      5  I highly recommend these candies, they are tha...\n",
      "3215  B005K4Q1VI      5  This was the best purchase that I could have m...\n",
      "2429  B0089SPDUW      4  I like strong coffee, but I don't want it to t...\n",
      "9613  B004JHIBV0      4  I thought that I should post a review in respo...\n",
      "5886  B003KLSZGW      5  I eat them just like this!  I like something w...\n",
      "5805  B0006J6GWG      5  At an early age my Beagle (yes, a beagle) was ...\n",
      "4149  B001EW5YQS      1  I ordered this item on March 22nd.  I expect t...\n",
      "3098  B001IZIC7E      5  These taste great and have a great texture.<br...\n",
      "1771  B001RVFDOO      5  These are better than any low calorie chips I'...\n",
      "7407  B0002Q1X6C      1  There was absolutly no scent or flavor to this...\n",
      "2017  B001E5E29A      5  Makes perfect, delicious, crisp on the outside...\n",
      "\n",
      "[10000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Loading our datafile as a DataFrame with pandas\n",
    "\n",
    "df = pd.read_csv('/Users/laniepreston/TMCI_Edits/group_assignment/FullReviews.csv' , usecols=[1,6,9], header=0)\n",
    "shuf_red_df = shuffle(df[:10000])\n",
    "print(shuf_red_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PoS-tagging our dataset\n",
    "train_df = shuf_red_df[:8000]\n",
    "test_df = shuf_red_df[:2000]\n",
    "\n",
    "\n",
    "train_tagged = []\n",
    "test_tagged = []\n",
    "\n",
    "\n",
    "for index, item in train_df.iterrows():\n",
    "    product_id = item[0]\n",
    "    score = item[1]\n",
    "    review = item[2]\n",
    "    review = review.split()\n",
    "    rev = []\n",
    "    for i in review:\n",
    "        rev += nltk.pos_tag([i], tagset=\"universal\")\n",
    "    train_tagged.append([rev, score])\n",
    "\n",
    "for index, item in test_df.iterrows():\n",
    "    product_id = item[0]\n",
    "    score = item[1]\n",
    "    review = item[2]\n",
    "    review = review.split()\n",
    "    rev = []\n",
    "    for i in review:\n",
    "        rev += nltk.pos_tag([i], tagset=\"universal\")\n",
    "    test_tagged.append([rev, score])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooccs_text = []\n",
    "for index, item in shuf_red_df.iterrows():\n",
    "    product_id = item[0]\n",
    "    score = item[1]\n",
    "    review = item[2]\n",
    "    review = review.split()\n",
    "    rev = []\n",
    "    for i in review:\n",
    "        rev += nltk.pos_tag([i], tagset=\"universal\")\n",
    "        cooccs_text.append([product_id, rev, score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "# Lemmatizing our dataset + filtering for stopwords (except negation)\n",
    "training_dicts = dict((k,[]) for k in [1,2,3,4,5])\n",
    "test_dicts = dict((k,[]) for k in [1,2,3,4,5])\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "un2wn_mapping = {\"VERB\" : wn.VERB, \"NOUN\" : wn.NOUN, \"ADJ\" : wn.ADJ, \"ADV\" : wn.ADV}\n",
    "\n",
    "lemmatized = []\n",
    "\n",
    "for review in train_tagged:\n",
    "    rev = review[0]\n",
    "    score = review[1]\n",
    "    lem_rev = []\n",
    "    for tup in rev:\n",
    "        w, tag = tup\n",
    "        \n",
    "        if len(w) > 1 and w.isalpha():\n",
    "            w = w.lower()\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        if tag in [\".\", \"X\"]:\n",
    "            continue\n",
    "        elif w.lower() in stopwords:\n",
    "            if w.lower() in [\"not\", \"t\", \"no\"]:  \n",
    "                lemma = w.lower()\n",
    "                tag = \"NEGATION\"\n",
    "            else: \n",
    "                continue\n",
    "        \n",
    "        elif tag in un2wn_mapping.keys():\n",
    "            lemma = nltk.WordNetLemmatizer().lemmatize(w, pos = un2wn_mapping[tag])\n",
    "        else:\n",
    "            lemma = nltk.WordNetLemmatizer().lemmatize(w)\n",
    "        lem_rev.append(\"-\".join([lemma, tag]))\n",
    "    training_dicts[score].append(lem_rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for review in test_tagged:\n",
    "    rev = review[0]\n",
    "    score = review[1]\n",
    "    lem_rev = []\n",
    "    for tup in rev:\n",
    "        w, tag = tup\n",
    "        \n",
    "        if len(w) > 1 and w.isalpha():\n",
    "            w = w.lower()\n",
    "        else:\n",
    "            continue\n",
    "        if tag in [\".\", \"X\"]:\n",
    "            continue\n",
    "        elif w.lower() in stopwords:\n",
    "            if w.lower() in [\"not\", \"t\", \"no\"]:  \n",
    "                lemma = w.lower()\n",
    "                tag = \"NEGATION\"\n",
    "                lem_rev.append(\"-\".join([lemma, tag]))\n",
    "            else: \n",
    "                continue\n",
    "        elif tag in un2wn_mapping.keys():\n",
    "            lemma = nltk.WordNetLemmatizer().lemmatize(w, pos = un2wn_mapping[tag])\n",
    "        else:\n",
    "            lemma = nltk.WordNetLemmatizer().lemmatize(w)\n",
    "        lem_rev.append(\"-\".join([lemma, tag]))\n",
    "    test_dicts[score].append(lem_rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooccs_dict = {}\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "un2wn_mapping = {\"VERB\" : wn.VERB, \"NOUN\" : wn.NOUN, \"ADJ\" : wn.ADJ, \"ADV\" : wn.ADV}\n",
    "\n",
    "for review in cooccs_text:\n",
    "    product_id = review[0]\n",
    "    rev = review[1]\n",
    "    score = review[2]\n",
    "    lem_rev = []\n",
    "    \n",
    "    for tup in rev:\n",
    "        w, tag = tup\n",
    "        \n",
    "        if len(w) > 1 and w.isalpha():\n",
    "            w = w.lower()\n",
    "        else:\n",
    "            continue\n",
    "        if tag in [\".\", \"X\"]:\n",
    "            continue\n",
    "        elif w.lower() in stopwords:\n",
    "            if w.lower() in [\"not\", \"t\", \"no\"]:  \n",
    "                lemma = w.lower()\n",
    "                tag = \"NEGATION\"\n",
    "                lem_rev.append(\"-\".join([lemma, tag]))\n",
    "            else: \n",
    "                continue\n",
    "        elif tag in un2wn_mapping.keys():\n",
    "            lemma = nltk.WordNetLemmatizer().lemmatize(w, pos = un2wn_mapping[tag])\n",
    "        else:\n",
    "            lemma = nltk.WordNetLemmatizer().lemmatize(w)\n",
    "        lem_rev.append(\"-\".join([lemma, tag]))\n",
    "        \n",
    "    cooccs_dict[product_id] = (score, lem_rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Save lemmatized text to a pickle file\n",
    "pickle.dump(training_dicts, open(\"training_dicts.txt\", \"wb\"))\n",
    "pickle.dump(test_dicts, open(\"test_dicts.txt\", \"wb\"))\n",
    "pickle.dump(cooccs_dict, open(\"cooccs_dict.txt\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['B00474H936', [('I', 'PRON'), ('have', 'VERB'), ('tried', 'VERB'), ('almost', 'ADV'), ('every', 'DET'), ('kind', 'NOUN'), ('of', 'ADP'), ('\"fake', 'NOUN'), ('meat\"', 'NOUN'), ('product', 'NOUN'), ('out', 'ADP'), ('there.', 'NOUN'), ('While', 'ADP'), ('I', 'PRON'), ('really', 'ADV'), ('like', 'ADP'), ('the', 'DET'), ('Morning', 'VERB'), ('Stars', 'NOUN'), ('and', 'CONJ'), ('the', 'DET'), ('Boca', 'NOUN'), ('\"ground', 'NOUN'), ('meat\"', 'NOUN'), ('you', 'PRON'), (\"can't\", 'NOUN'), ('really', 'ADV'), ('make', 'VERB'), ('them', 'PRON'), ('into', 'ADP'), ('anything.<br', 'NOUN'), ('/><br', 'NOUN'), ('/>This', 'NOUN'), ('product', 'NOUN'), ('was', 'VERB'), ('great.', 'NOUN'), ('Soak', 'NOUN'), ('it', 'PRON'), ('for', 'ADP'), ('15', 'NUM'), ('minutes', 'NOUN'), ('in', 'ADP'), ('water,', 'NOUN'), ('and', 'CONJ'), ('viola', 'NOUN'), ('you', 'PRON'), ('have', 'VERB'), ('a', 'DET'), ('really', 'ADV'), ('good', 'ADJ'), ('ground', 'NOUN'), ('meat', 'NOUN'), ('you', 'PRON'), ('can', 'VERB'), ('make', 'VERB'), ('something', 'NOUN'), ('out', 'ADP'), ('of,', 'NOUN'), ('The', 'DET'), ('only', 'ADV'), ('problem', 'NOUN'), ('is', 'VERB'), ('that', 'ADP'), ('it', 'PRON'), ('tends', 'NOUN'), ('to', 'PRT'), ('be', 'VERB'), ('a', 'DET'), ('little', 'ADJ'), ('saly', 'NOUN'), ('for', 'ADP'), ('my', 'PRON'), ('taste,', 'NOUN'), ('so', 'ADV'), ('be', 'VERB'), ('carefull', 'NOUN'), ('when', 'ADV'), ('you', 'PRON'), ('are', 'VERB'), ('seasoning', 'VERB'), ('it.', 'NOUN')], 4], ['B00474H936', [('I', 'PRON'), ('have', 'VERB'), ('tried', 'VERB'), ('almost', 'ADV'), ('every', 'DET'), ('kind', 'NOUN'), ('of', 'ADP'), ('\"fake', 'NOUN'), ('meat\"', 'NOUN'), ('product', 'NOUN'), ('out', 'ADP'), ('there.', 'NOUN'), ('While', 'ADP'), ('I', 'PRON'), ('really', 'ADV'), ('like', 'ADP'), ('the', 'DET'), ('Morning', 'VERB'), ('Stars', 'NOUN'), ('and', 'CONJ'), ('the', 'DET'), ('Boca', 'NOUN'), ('\"ground', 'NOUN'), ('meat\"', 'NOUN'), ('you', 'PRON'), (\"can't\", 'NOUN'), ('really', 'ADV'), ('make', 'VERB'), ('them', 'PRON'), ('into', 'ADP'), ('anything.<br', 'NOUN'), ('/><br', 'NOUN'), ('/>This', 'NOUN'), ('product', 'NOUN'), ('was', 'VERB'), ('great.', 'NOUN'), ('Soak', 'NOUN'), ('it', 'PRON'), ('for', 'ADP'), ('15', 'NUM'), ('minutes', 'NOUN'), ('in', 'ADP'), ('water,', 'NOUN'), ('and', 'CONJ'), ('viola', 'NOUN'), ('you', 'PRON'), ('have', 'VERB'), ('a', 'DET'), ('really', 'ADV'), ('good', 'ADJ'), ('ground', 'NOUN'), ('meat', 'NOUN'), ('you', 'PRON'), ('can', 'VERB'), ('make', 'VERB'), ('something', 'NOUN'), ('out', 'ADP'), ('of,', 'NOUN'), ('The', 'DET'), ('only', 'ADV'), ('problem', 'NOUN'), ('is', 'VERB'), ('that', 'ADP'), ('it', 'PRON'), ('tends', 'NOUN'), ('to', 'PRT'), ('be', 'VERB'), ('a', 'DET'), ('little', 'ADJ'), ('saly', 'NOUN'), ('for', 'ADP'), ('my', 'PRON'), ('taste,', 'NOUN'), ('so', 'ADV'), ('be', 'VERB'), ('carefull', 'NOUN'), ('when', 'ADV'), ('you', 'PRON'), ('are', 'VERB'), ('seasoning', 'VERB'), ('it.', 'NOUN')], 4], ['B00474H936', [('I', 'PRON'), ('have', 'VERB'), ('tried', 'VERB'), ('almost', 'ADV'), ('every', 'DET'), ('kind', 'NOUN'), ('of', 'ADP'), ('\"fake', 'NOUN'), ('meat\"', 'NOUN'), ('product', 'NOUN'), ('out', 'ADP'), ('there.', 'NOUN'), ('While', 'ADP'), ('I', 'PRON'), ('really', 'ADV'), ('like', 'ADP'), ('the', 'DET'), ('Morning', 'VERB'), ('Stars', 'NOUN'), ('and', 'CONJ'), ('the', 'DET'), ('Boca', 'NOUN'), ('\"ground', 'NOUN'), ('meat\"', 'NOUN'), ('you', 'PRON'), (\"can't\", 'NOUN'), ('really', 'ADV'), ('make', 'VERB'), ('them', 'PRON'), ('into', 'ADP'), ('anything.<br', 'NOUN'), ('/><br', 'NOUN'), ('/>This', 'NOUN'), ('product', 'NOUN'), ('was', 'VERB'), ('great.', 'NOUN'), ('Soak', 'NOUN'), ('it', 'PRON'), ('for', 'ADP'), ('15', 'NUM'), ('minutes', 'NOUN'), ('in', 'ADP'), ('water,', 'NOUN'), ('and', 'CONJ'), ('viola', 'NOUN'), ('you', 'PRON'), ('have', 'VERB'), ('a', 'DET'), ('really', 'ADV'), ('good', 'ADJ'), ('ground', 'NOUN'), ('meat', 'NOUN'), ('you', 'PRON'), ('can', 'VERB'), ('make', 'VERB'), ('something', 'NOUN'), ('out', 'ADP'), ('of,', 'NOUN'), ('The', 'DET'), ('only', 'ADV'), ('problem', 'NOUN'), ('is', 'VERB'), ('that', 'ADP'), ('it', 'PRON'), ('tends', 'NOUN'), ('to', 'PRT'), ('be', 'VERB'), ('a', 'DET'), ('little', 'ADJ'), ('saly', 'NOUN'), ('for', 'ADP'), ('my', 'PRON'), ('taste,', 'NOUN'), ('so', 'ADV'), ('be', 'VERB'), ('carefull', 'NOUN'), ('when', 'ADV'), ('you', 'PRON'), ('are', 'VERB'), ('seasoning', 'VERB'), ('it.', 'NOUN')], 4], ['B00474H936', [('I', 'PRON'), ('have', 'VERB'), ('tried', 'VERB'), ('almost', 'ADV'), ('every', 'DET'), ('kind', 'NOUN'), ('of', 'ADP'), ('\"fake', 'NOUN'), ('meat\"', 'NOUN'), ('product', 'NOUN'), ('out', 'ADP'), ('there.', 'NOUN'), ('While', 'ADP'), ('I', 'PRON'), ('really', 'ADV'), ('like', 'ADP'), ('the', 'DET'), ('Morning', 'VERB'), ('Stars', 'NOUN'), ('and', 'CONJ'), ('the', 'DET'), ('Boca', 'NOUN'), ('\"ground', 'NOUN'), ('meat\"', 'NOUN'), ('you', 'PRON'), (\"can't\", 'NOUN'), ('really', 'ADV'), ('make', 'VERB'), ('them', 'PRON'), ('into', 'ADP'), ('anything.<br', 'NOUN'), ('/><br', 'NOUN'), ('/>This', 'NOUN'), ('product', 'NOUN'), ('was', 'VERB'), ('great.', 'NOUN'), ('Soak', 'NOUN'), ('it', 'PRON'), ('for', 'ADP'), ('15', 'NUM'), ('minutes', 'NOUN'), ('in', 'ADP'), ('water,', 'NOUN'), ('and', 'CONJ'), ('viola', 'NOUN'), ('you', 'PRON'), ('have', 'VERB'), ('a', 'DET'), ('really', 'ADV'), ('good', 'ADJ'), ('ground', 'NOUN'), ('meat', 'NOUN'), ('you', 'PRON'), ('can', 'VERB'), ('make', 'VERB'), ('something', 'NOUN'), ('out', 'ADP'), ('of,', 'NOUN'), ('The', 'DET'), ('only', 'ADV'), ('problem', 'NOUN'), ('is', 'VERB'), ('that', 'ADP'), ('it', 'PRON'), ('tends', 'NOUN'), ('to', 'PRT'), ('be', 'VERB'), ('a', 'DET'), ('little', 'ADJ'), ('saly', 'NOUN'), ('for', 'ADP'), ('my', 'PRON'), ('taste,', 'NOUN'), ('so', 'ADV'), ('be', 'VERB'), ('carefull', 'NOUN'), ('when', 'ADV'), ('you', 'PRON'), ('are', 'VERB'), ('seasoning', 'VERB'), ('it.', 'NOUN')], 4], ['B00474H936', [('I', 'PRON'), ('have', 'VERB'), ('tried', 'VERB'), ('almost', 'ADV'), ('every', 'DET'), ('kind', 'NOUN'), ('of', 'ADP'), ('\"fake', 'NOUN'), ('meat\"', 'NOUN'), ('product', 'NOUN'), ('out', 'ADP'), ('there.', 'NOUN'), ('While', 'ADP'), ('I', 'PRON'), ('really', 'ADV'), ('like', 'ADP'), ('the', 'DET'), ('Morning', 'VERB'), ('Stars', 'NOUN'), ('and', 'CONJ'), ('the', 'DET'), ('Boca', 'NOUN'), ('\"ground', 'NOUN'), ('meat\"', 'NOUN'), ('you', 'PRON'), (\"can't\", 'NOUN'), ('really', 'ADV'), ('make', 'VERB'), ('them', 'PRON'), ('into', 'ADP'), ('anything.<br', 'NOUN'), ('/><br', 'NOUN'), ('/>This', 'NOUN'), ('product', 'NOUN'), ('was', 'VERB'), ('great.', 'NOUN'), ('Soak', 'NOUN'), ('it', 'PRON'), ('for', 'ADP'), ('15', 'NUM'), ('minutes', 'NOUN'), ('in', 'ADP'), ('water,', 'NOUN'), ('and', 'CONJ'), ('viola', 'NOUN'), ('you', 'PRON'), ('have', 'VERB'), ('a', 'DET'), ('really', 'ADV'), ('good', 'ADJ'), ('ground', 'NOUN'), ('meat', 'NOUN'), ('you', 'PRON'), ('can', 'VERB'), ('make', 'VERB'), ('something', 'NOUN'), ('out', 'ADP'), ('of,', 'NOUN'), ('The', 'DET'), ('only', 'ADV'), ('problem', 'NOUN'), ('is', 'VERB'), ('that', 'ADP'), ('it', 'PRON'), ('tends', 'NOUN'), ('to', 'PRT'), ('be', 'VERB'), ('a', 'DET'), ('little', 'ADJ'), ('saly', 'NOUN'), ('for', 'ADP'), ('my', 'PRON'), ('taste,', 'NOUN'), ('so', 'ADV'), ('be', 'VERB'), ('carefull', 'NOUN'), ('when', 'ADV'), ('you', 'PRON'), ('are', 'VERB'), ('seasoning', 'VERB'), ('it.', 'NOUN')], 4]]\n"
     ]
    }
   ],
   "source": [
    "print(cooccs_text[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
