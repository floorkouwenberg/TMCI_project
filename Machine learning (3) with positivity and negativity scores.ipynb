{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning with positivity and negativity scores\n",
    "\n",
    "This notebook contains a Naive Bayes model trained on the positivity and negativity scores per review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import naive_bayes, metrics\n",
    "from itertools import chain\n",
    "from math import log\n",
    "from nltk import BigramAssocMeasures\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet as wn \n",
    "import operator \n",
    "\n",
    "#importing the test and train dataset\n",
    "with open('test_dicts.txt', 'rb') as file:\n",
    "    test_lemmedreviews = pickle.load(file)\n",
    "    \n",
    "with open('training_dicts.txt', 'rb') as file:\n",
    "    train_lemmedreviews = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {'NOUN':wn.NOUN, \"ADJ\": wn.ADJ, 'VERB' : wn.VERB, \"ADV\": wn.ADV}\n",
    "\n",
    "def find_senti(lst):\n",
    "    \"\"\"Finds the positivity and negativity score per review\"\"\"\n",
    "    avg_review = []\n",
    "    for sublist in lst:\n",
    "        avg_positivity, avg_negativity, total_words = 0, 0, 0\n",
    "        #find total num words in each class for calculating the average\n",
    "        total_words = len(sublist)\n",
    "        for word in sublist:\n",
    "            #split item into individual word and lemma \n",
    "            trunc_word = word.split(\"-\")\n",
    "            if trunc_word[1] not in mapping:\n",
    "                continue\n",
    "            #get the right part of speech from predefined mapping\n",
    "            new_pos = mapping[trunc_word[1]]\n",
    "            if len(list(swn.senti_synsets(trunc_word[0], pos = new_pos))) > 0:\n",
    "                scores = swn.senti_synset(trunc_word[0] + \".\"+ new_pos + \".01\")\n",
    "                #find senti-wordnet's positivity and negativity rating for each word in each class  \n",
    "                avg_positivity += scores.pos_score()\n",
    "                avg_negativity += scores.neg_score() \n",
    "        avg_tupl = ((avg_positivity/total_words), (avg_negativity/total_words))\n",
    "        avg_review.append(avg_tupl)\n",
    "                \n",
    "                \n",
    "\n",
    "    return avg_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For all reviews in the training set find their positivity and negativity scores\n",
    "all_scores=[]\n",
    "for i in range(1,6): \n",
    "    all_scores += find_senti(train_lemmedreviews[i])\n",
    " \n",
    "#Create the label vector\n",
    "labelsVec = np.zeros((sum([len(v) for v in train_lemmedreviews.values()])))\n",
    "docId = 0\n",
    "for score in range(1, 6):\n",
    "    for rev in train_lemmedreviews[score]:\n",
    "        labelsVec[docId] = score\n",
    "        docId += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create the model based on the positivity and negativity scores\n",
    "clf = naive_bayes.MultinomialNB()\n",
    "clf.fit(all_scores, labelsVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the test scores and the golden standard\n",
    "all_scores_test=[]\n",
    "for i in range(1,6): \n",
    "    all_scores_test += find_senti(test_lemmedreviews[i])   \n",
    "\n",
    "goldStandard = np.zeros((sum([len(v) for v in test_lemmedreviews.values()])))\n",
    "docId = 0\n",
    "for score in range(1, 6):\n",
    "    for rev in test_lemmedreviews[score]:\n",
    "        goldStandard[docId] = score\n",
    "        docId += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a prediction\n",
    "predicted = clf.predict(all_scores_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.6115\n",
      "precision: 0.2\n",
      "recall: 0.1223\n",
      "f1-measure: 0.15178405212534907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\floor kouwenberg\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "c:\\users\\floor kouwenberg\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# accuracy\n",
    "print(\"accuracy:\", metrics.accuracy_score(predicted, goldStandard))\n",
    "\n",
    "# precision, recall and f-measure\n",
    "print(\"precision:\", metrics.precision_score(predicted, goldStandard, average='macro'))\n",
    "print(\"recall:\", metrics.recall_score(predicted, goldStandard, average='macro'))\n",
    "print(\"f1-measure:\", metrics.f1_score(predicted, goldStandard, average='macro'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
