{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We thought that finding the co-occurrences of words in the review with negative or positive words from the corpus would be a good way to find out exactly which parts of the purchased product make it good or bad. For example, in a hot sauce review, it would be helpful for manufacturers and consumers alike to understand if it's the flavor, packaging, or something else that make it popular or unpopular. We could use this both to be able to predict the number of stars that an Amazon review will recieve and to indicate to customers what the best parts of a product are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "with open('training_dicts.txt', 'rb') as file:\n",
    "    lemmedreviews = pickle.load(file)\n",
    "    \n",
    "#make lists for each class in training set \n",
    "one_star = lemmedreviews[1]\n",
    "two_star = lemmedreviews[2]\n",
    "three_star = lemmedreviews[3]\n",
    "four_star = lemmedreviews[4]\n",
    "five_star = lemmedreviews[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('taste-NOUN', 'like-ADP'), 88), (('not-NEGATION', 'would-VERB'), 73), (('not-NEGATION', 'product-NOUN'), 70), (('not-NEGATION', 'like-ADP'), 65), (('taste-NOUN', 'not-NEGATION'), 51), (('not-NEGATION', 'even-ADV'), 46), (('not-NEGATION', 'buy-VERB'), 36), (('not-NEGATION', 'good-ADJ'), 32), (('taste-VERB', 'like-ADP'), 29), (('not-NEGATION', 'worth-NOUN'), 27)]\n",
      "[(('like-ADP', 'taste-NOUN'), 66), (('like-ADP', 'not-NEGATION'), 56), (('not-NEGATION', 'taste-NOUN'), 54), (('would-VERB', 'not-NEGATION'), 37), (('like-ADP', 'coffee-NOUN'), 37), (('not-NEGATION', 'good-ADJ'), 35), (('not-NEGATION', 'product-NOUN'), 25), (('not-NEGATION', 'flavor-NOUN'), 24), (('like-ADP', 'really-ADV'), 23), (('not-NEGATION', 'worth-NOUN'), 22)]\n",
      "[(('taste-NOUN', 'like-ADP'), 88), (('would-VERB', 'not-NEGATION'), 65), (('not-NEGATION', 'taste-NOUN'), 61), (('not-NEGATION', 'like-ADP'), 60), (('coffee-NOUN', 'not-NEGATION'), 58), (('not-NEGATION', 'good-ADJ'), 53), (('not-NEGATION', 'really-ADV'), 50), (('one-NUM', 'not-NEGATION'), 44), (('really-ADV', 'like-ADP'), 43), (('coffee-NOUN', 'like-ADP'), 36)]\n",
      "[(('taste-NOUN', 'like-ADP'), 119), (('not-NEGATION', 'good-ADJ'), 84), (('like-ADP', 'not-NEGATION'), 81), (('like-ADP', 'coffee-NOUN'), 63), (('not-NEGATION', 'taste-NOUN'), 57), (('not-NEGATION', 'flavor-NOUN'), 56), (('not-NEGATION', 'coffee-NOUN'), 53), (('taste-NOUN', 'good-ADJ'), 52), (('coffee-NOUN', 'cup-NOUN'), 51), (('not-NEGATION', 'sure-NOUN'), 46)]\n",
      "[(('like-ADP', 'taste-NOUN'), 251), (('like-ADP', 'not-NEGATION'), 207), (('highly-ADV', 'recommend-NOUN'), 177), (('dog-NOUN', 'food-NOUN'), 174), (('taste-NOUN', 'not-NEGATION'), 157), (('really-ADV', 'like-ADP'), 151), (('flavor-NOUN', 'not-NEGATION'), 147), (('coffee-NOUN', 'not-NEGATION'), 142), (('best-ADJ', 'ever-ADV'), 134), (('cup-NOUN', 'coffee-NOUN'), 133)]\n"
     ]
    }
   ],
   "source": [
    "#find co-occurences \n",
    "from collections import Counter\n",
    "\n",
    "def find_cooccs(lst):\n",
    "    span = 3\n",
    "    cooccs_stem_surface = Counter()\n",
    "\n",
    "    for sentence in lst:\n",
    "        for i,w in enumerate(sentence):\n",
    "            #check all co-occurring words within a 3 word span \n",
    "            span_range = list(range(max(i- span, 0), i)) \n",
    "            span_range.extend(range(i+1, min(i + span + 1, len(sentence)))) \n",
    "            for cw in [sentence[idx] for idx in span_range]:\n",
    "                if cw != w and (cw, w) not in cooccs_stem_surface:\n",
    "                    cooccs_stem_surface[(w, cw)] += 1\n",
    "    \n",
    "    print(cooccs_stem_surface.most_common(10))\n",
    "    return cooccs_stem_surface \n",
    "\n",
    "#apply to every class \n",
    "one_cooccs = find_cooccs(one_star)\n",
    "two_cooccs = find_cooccs(two_star)\n",
    "three_cooccs = find_cooccs(three_star)\n",
    "four_cooccs = find_cooccs(four_star)\n",
    "five_cooccs = find_cooccs(five_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the most \"polarized\" combinations of words per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform sentiment analysis\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet as wn \n",
    "import operator \n",
    "import collections\n",
    "\n",
    "#create mapping for proper sentiment analysis \n",
    "mapping = {'NOUN':wn.NOUN, \"ADJ\": wn.ADJ, 'VERB' : wn.VERB}\n",
    "\n",
    "def find_vals(lst):\n",
    "    #make synsets out of tuples\n",
    "    list_cooccs = lst\n",
    "    word1, word2 = \"\", \"\"\n",
    "    positive_score, negative_score, positive_score2, negative_score2 = 0,0,0,0\n",
    "    #dictionary to store tuples of co-occurences and their positivity \n",
    "    wordvals = {}\n",
    "\n",
    "    #give items new mapping for use with senti_synsets\n",
    "    for tup in list_cooccs:\n",
    "        #get individual words out of tuple\n",
    "        w1 = tup[0].split(\"-\")\n",
    "        w2 = tup[1].split(\"-\")\n",
    "    \n",
    "        #check if first word in tuple is contained within mapping\n",
    "        if w1[1] in mapping.keys() and mapping[w1[1]] == 'a':\n",
    "            #extract part of speech\n",
    "            one_pos = mapping[w1[1]]\n",
    "            #extract the word itself \n",
    "            word1 = w1[0]\n",
    "        \n",
    "            #test that synsets exist \n",
    "            if len(list(swn.senti_synsets(word1, pos = one_pos))) > 0:\n",
    "                scores = swn.senti_synset(word1 + \".\"+ one_pos + \".01\")\n",
    "                #calculate positive and negative scores for word 1 \n",
    "                positive_score = scores.pos_score()\n",
    "                negative_score = scores.neg_score()\n",
    "            \n",
    "        #repeat for second word in tuple         \n",
    "        if w2[1] in mapping.keys() and mapping[w2[1]] == 'n':\n",
    "            #extract part of speech \n",
    "            two_pos = mapping[w2[1]]\n",
    "            word2 = w2[0]\n",
    "        \n",
    "            if len(list(swn.senti_synsets(word2, pos = two_pos))) > 0:\n",
    "                scores2 = swn.senti_synset(word2 + \".\"+ two_pos + \".01\")\n",
    "                #calculate positive and negative scores for word 1 \n",
    "                positive_score2 = scores2.pos_score()\n",
    "                negative_score2 = scores2.neg_score() \n",
    "            \n",
    "        #store tuples and positivity values in a dictionary\n",
    "        tupl = (word1, word2)\n",
    "        wordvals[tupl] = (positive_score + negative_score + positive_score2 + negative_score2)\n",
    "\n",
    "    #sort the dictionary of positive and negative values from largest to smallest \n",
    "    sorted_wordvals = list(reversed(sorted(wordvals.items(), key=operator.itemgetter(1))))\n",
    "    #sorted_dict = collections.OrderedDict(reversed(sorted_wordvals))\n",
    "    return sorted_wordvals\n",
    "\n",
    "one_vals = find_vals(one_cooccs)\n",
    "two_vals = find_vals(two_cooccs)\n",
    "three_vals = find_vals(three_cooccs)\n",
    "four_vals = find_vals(four_cooccs)\n",
    "five_vals = find_vals(five_cooccs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('true', 'wonder'), 1.625), (('easy', 'disappointment'), 1.625), (('fresh', 'problem'), 1.625), (('safe', 'stuck'), 1.625), (('safe', 'regret'), 1.625), (('gracious', 'wolfgang'), 1.625), (('gracious', 'disappointment'), 1.625), (('different', 'difference'), 1.5), (('different', 'explain'), 1.5), (('nice', 'credit'), 1.5)]\n",
      "[(('nice', 'health'), 1.5), (('artificial', 'bliss'), 1.5), (('delicious', 'emergency'), 1.5), (('weak', 'regret'), 1.5), (('happy', 'dark'), 1.5), (('true', 'dark'), 1.5), (('weak', 'reserve'), 1.5), (('weak', 'oz'), 1.5), (('poor', 'drawback'), 1.5), (('negative', 'disease'), 1.5)]\n",
      "[(('fresh', 'drawback'), 1.5), (('easy', 'problem'), 1.5), (('attractive', 'problem'), 1.5), (('beneficial', 'difference'), 1.5), (('unnatural', 'adhd'), 1.5), (('fresh', 'seem'), 1.5), (('fresh', 'grind'), 1.5), (('good', 'disproportional'), 1.5), (('good', 'complaint'), 1.5), (('worried', 'dark'), 1.5)]\n",
      "[(('fresh', 'compare'), 1.75), (('wrong', 'reserve'), 1.75), (('best', 'reserve'), 1.625), (('concerned', 'cheap'), 1.625), (('concerned', 'complaint'), 1.625), (('fresh', 'problem'), 1.625), (('poor', 'problem'), 1.625), (('nice', 'complaint'), 1.625), (('nice', 'emergency'), 1.625), (('nice', 'unique'), 1.625)]\n",
      "[(('fresh', 'emergency'), 1.75), (('true', 'yahoo'), 1.75), (('good', 'kudos'), 1.75), (('worried', 'difference'), 1.75), (('nice', 'lightness'), 1.75), (('nice', 'adore'), 1.75), (('healthy', 'kudos'), 1.75), (('fresh', 'eat'), 1.75), (('fresh', 'scent'), 1.75), (('fresh', 'accident'), 1.75)]\n"
     ]
    }
   ],
   "source": [
    "print(one_vals[10:20])\n",
    "print(two_vals[10:20])\n",
    "print(three_vals[10:20])\n",
    "print(four_vals[10:20])\n",
    "print(five_vals[10:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to find the PLMI of the co-occurences per class to use as a feature for classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plmi measure\n",
    "from itertools import chain\n",
    "from math import log\n",
    "\n",
    "\n",
    "def ppmi(o_11, r_1, c_1, n):\n",
    "    \"\"\"\n",
    "    Positive Pointwise Mutual Information (Church & Hanks, 1990)\n",
    "    \n",
    "    PMI is also available in NLTK:\n",
    "    from nltk.metrics import BigramAssocMeasures\n",
    "    print BigramAssocMeasures.pmi(8, (15828, 4675), 14307668)\n",
    "    \"\"\"\n",
    "    observed = o_11\n",
    "    expected = (r_1*c_1)/n \n",
    "    res = log(observed/expected,2)\n",
    "    return max(0, res)\n",
    "\n",
    "def plmi(o_11, r_1, c_1, n):\n",
    "    \"\"\"\n",
    "    Positive Local Mutual Information, useful for leveraging the \n",
    "    low-frequency bias of the PPMI\n",
    "    \"\"\"\n",
    "    res = o_11 * ppmi(o_11, r_1, c_1, n)\n",
    "    return res\n",
    "\n",
    "def find_plmi(cooccs_dict, lst):\n",
    "    N = len(cooccs_dict.values())\n",
    "    plmis_stem_surface = Counter()\n",
    "    stemmed_frequencies = Counter(chain(*lst))\n",
    "\n",
    "\n",
    "    for k,v in cooccs_dict.items():\n",
    "        plmis_stem_surface[k] = plmi(v, stemmed_frequencies[k[0]], stemmed_frequencies[k[1]], N)\n",
    "\n",
    "    print(plmis_stem_surface.most_common(10))\n",
    "    return plmis_stem_surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('taste-NOUN', 'like-ADP'), 498.21421039440526), (('not-NEGATION', 'would-VERB'), 328.187444872234), (('not-NEGATION', 'product-NOUN'), 315.9453669160056), (('not-NEGATION', 'like-ADP'), 245.1104691952661), (('not-NEGATION', 'even-ADV'), 206.80891232686838), (('taste-NOUN', 'not-NEGATION'), 203.19398438910855), (('wolfgang-NOUN', 'puck-NOUN'), 174.8994910957734), (('customer-NOUN', 'service-NOUN'), 169.3327182410485), (('bad-ADJ', 'ever-ADV'), 168.73227297299397), (('taste-VERB', 'like-ADP'), 165.09501887758555)]\n",
      "[(('like-ADP', 'taste-NOUN'), 346.7954123964976), (('like-ADP', 'not-NEGATION'), 223.73486055771255), (('not-NEGATION', 'taste-NOUN'), 223.2276969716608), (('like-ADP', 'coffee-NOUN'), 199.5862918162832), (('would-VERB', 'not-NEGATION'), 159.49068237603507), (('not-NEGATION', 'good-ADJ'), 158.6901697871882), (('hot-ADJ', 'chocolate-NOUN'), 152.92386027809886), (('bake-VERB', 'lay-NOUN'), 146.25097065400752), (('not-NEGATION', 'worth-NOUN'), 134.97016198705478), (('like-ADP', 'really-ADV'), 118.71462239993872)]\n",
      "[(('taste-NOUN', 'like-ADP'), 458.9813939738063), (('green-ADJ', 'mountain-NOUN'), 314.04695550132635), (('would-VERB', 'not-NEGATION'), 284.7492455096631), (('wolfgang-NOUN', 'puck-NOUN'), 280.51201695688735), (('not-NEGATION', 'taste-NOUN'), 238.1640849611903), (('not-NEGATION', 'good-ADJ'), 233.30346113041387), (('orange-NOUN', 'tangerine-NOUN'), 226.90103165010166), (('not-NEGATION', 'really-ADV'), 226.42834163922228), (('coffee-NOUN', 'not-NEGATION'), 224.7885991975894), (('really-ADV', 'like-ADP'), 219.0269290921876)]\n",
      "[(('taste-NOUN', 'like-ADP'), 598.1931260359395), (('not-NEGATION', 'good-ADJ'), 351.6930290091218), (('like-ADP', 'not-NEGATION'), 299.4132795281518), (('wolfgang-NOUN', 'puck-NOUN'), 281.6375485015434), (('coffee-NOUN', 'cup-NOUN'), 278.8983957935255), (('orange-NOUN', 'tangerine-NOUN'), 276.7538162230383), (('not-NEGATION', 'sure-NOUN'), 271.24761492801014), (('like-ADP', 'coffee-NOUN'), 261.6304425227654), (('give-VERB', 'star-NOUN'), 237.04790871378287), (('not-NEGATION', 'flavor-NOUN'), 225.78096684113694)]\n",
      "[(('highly-ADV', 'recommend-NOUN'), 1490.5335878678293), (('like-ADP', 'taste-NOUN'), 1203.2858617037896), (('dog-NOUN', 'food-NOUN'), 1130.5413679636213), (('best-ADJ', 'ever-ADV'), 949.4788504462299), (('gluten-NOUN', 'free-ADJ'), 880.6380947333453), (('grocery-NOUN', 'store-NOUN'), 856.3352128289264), (('like-ADP', 'not-NEGATION'), 790.1921636313737), (('would-VERB', 'recommend-NOUN'), 756.7204845318499), (('cup-NOUN', 'coffee-NOUN'), 729.9836513557714), (('potato-NOUN', 'chip-NOUN'), 713.4382581986213)]\n"
     ]
    }
   ],
   "source": [
    "one_plmi = find_plmi(one_cooccs, one_star)\n",
    "two_plmi =  find_plmi(two_cooccs, two_star)\n",
    "three_plmi = find_plmi(three_cooccs, three_star)\n",
    "four_plmi = find_plmi(four_cooccs, four_star)\n",
    "five_plmi = find_plmi(five_cooccs, five_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the average positivity and negativity of each class to use as features as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find average positivity + negativity for each word in each class \n",
    "def find_senti(lst):\n",
    "    avg_positivity, avg_negativity, total_words = 0, 0, 0\n",
    "    avg_review = []\n",
    "    \n",
    "    for sublist in lst:\n",
    "        #find total num words in each class for calculating the average\n",
    "        total_words += len(sublist)\n",
    "        avg_review = []\n",
    "        for word in sublist:\n",
    "            #split item into individual word and lemma \n",
    "            trunc_word = word.split(\"-\")\n",
    "            #TODO: handle negation!! this is a band-aid  \n",
    "            if trunc_word[1] not in mapping:\n",
    "                continue\n",
    "            #get the right part of speech from predefined mapping\n",
    "            new_pos = mapping[trunc_word[1]]\n",
    "            if len(list(swn.senti_synsets(trunc_word[0], pos = new_pos))) > 0:\n",
    "                scores = swn.senti_synset(trunc_word[0] + \".\"+ new_pos + \".01\")\n",
    "                #find senti-wordnet's positivity and negativity rating for each word in each class  \n",
    "                avg_positivity += scores.pos_score()\n",
    "                avg_negativity += scores.neg_score() \n",
    "                avg_tupl = ((avg_positivity/total_words), (avg_negativity/total_words))\n",
    "                avg_review.append(avg_tupl)\n",
    "                \n",
    "                \n",
    "\n",
    "    return avg_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.03653212884238064, 0.03473585676913015), (0.036537238391105294, 0.03473585676913015), (0.036537238391105294, 0.03473585676913015), (0.036537238391105294, 0.03473585676913015), (0.03654745748855461, 0.03474607586657946), (0.03654745748855461, 0.03474607586657946), (0.03654745748855461, 0.03474607586657946), (0.03654745748855461, 0.03474607586657946), (0.03654745748855461, 0.03474607586657946), (0.03654745748855461, 0.03474607586657946), (0.03654745748855461, 0.03476140451275343), (0.03654745748855461, 0.03476140451275343), (0.036552567037279264, 0.03476140451275343), (0.036552567037279264, 0.03477162361020275), (0.03655767658600392, 0.034786952256376714), (0.03655767658600392, 0.034786952256376714), (0.03655767658600392, 0.034786952256376714), (0.03655767658600392, 0.034786952256376714), (0.03655767658600392, 0.034786952256376714), (0.03655767658600392, 0.034807390451275344), (0.03657811478090255, 0.0348125), (0.03657811478090255, 0.0348125), (0.03657811478090255, 0.0348125), (0.03657811478090255, 0.034843157292347944), (0.03657811478090255, 0.034843157292347944), (0.036588333878351864, 0.034843157292347944)]\n",
      "[(0.04083144225920323, 0.03310167675239536), (0.04083144225920323, 0.03310167675239536), (0.04083144225920323, 0.03310167675239536), (0.04083144225920323, 0.03310167675239536), (0.04083144225920323, 0.03310167675239536), (0.04083144225920323, 0.03310167675239536), (0.04083932173474533, 0.033109556227937466), (0.04083932173474533, 0.033109556227937466), (0.04084720121028744, 0.03313319465456379), (0.040862960161371656, 0.033148953605648006), (0.040862960161371656, 0.033148953605648006), (0.040862960161371656, 0.033148953605648006), (0.04087871911245587, 0.033156833081190114), (0.04087871911245587, 0.033156833081190114), (0.04087871911245587, 0.033156833081190114), (0.04087871911245587, 0.033156833081190114), (0.04087871911245587, 0.033156833081190114), (0.04087871911245587, 0.033156833081190114), (0.04087871911245587, 0.033156833081190114), (0.04087871911245587, 0.033156833081190114), (0.04087871911245587, 0.033196230458900654), (0.04087871911245587, 0.033196230458900654), (0.04087871911245587, 0.033235627836611194), (0.04087871911245587, 0.033235627836611194), (0.04087871911245587, 0.033235627836611194)]\n",
      "[(0.04260330923829023, 0.03360422847114862), (0.04260330923829023, 0.03360422847114862), (0.04260853215225839, 0.03360422847114862), (0.04261897798019471, 0.03360422847114862), (0.04261897798019471, 0.03360422847114862), (0.04261897798019471, 0.03360422847114862), (0.04263464672209919, 0.03360422847114862), (0.04263464672209919, 0.03360422847114862), (0.04263464672209919, 0.03360422847114862), (0.04263464672209919, 0.03360422847114862), (0.04263464672209919, 0.03360422847114862), (0.04263464672209919, 0.03360422847114862), (0.04263464672209919, 0.03360422847114862), (0.04263464672209919, 0.03360422847114862), (0.042639869636067355, 0.03360422847114862), (0.04264509255003551, 0.03360422847114862), (0.04267120711987632, 0.03360422847114862), (0.04267120711987632, 0.03360422847114862), (0.04267120711987632, 0.03360422847114862), (0.04267120711987632, 0.03360422847114862), (0.04267120711987632, 0.03361467429908495), (0.04268165294781264, 0.03361467429908495), (0.042686875861780806, 0.03363034304098943), (0.04271821334558977, 0.03363034304098943), (0.04271821334558977, 0.03363034304098943), (0.04271821334558977, 0.03363034304098943), (0.04272865917352609, 0.03364078886892575), (0.04272865917352609, 0.03364078886892575), (0.04272865917352609, 0.03364078886892575)]\n",
      "[(0.04900066401062417, 0.030258964143426294), (0.04902390438247012, 0.030258964143426294), (0.04902390438247012, 0.030258964143426294), (0.04902390438247012, 0.030258964143426294), (0.04902390438247012, 0.030258964143426294), (0.04902390438247012, 0.030275564409030543), (0.04902390438247012, 0.030275564409030543), (0.04902390438247012, 0.030275564409030543), (0.04902390438247012, 0.030275564409030543), (0.04902390438247012, 0.030285524568393094), (0.04902390438247012, 0.030285524568393094), (0.04902390438247012, 0.030285524568393094), (0.04902390438247012, 0.030285524568393094), (0.04902390438247012, 0.030285524568393094), (0.04902390438247012, 0.030285524568393094), (0.049030544488711816, 0.030292164674634796), (0.049030544488711816, 0.030292164674634796), (0.049030544488711816, 0.030292164674634796), (0.049030544488711816, 0.030292164674634796), (0.049030544488711816, 0.030312084993359894), (0.049030544488711816, 0.030312084993359894), (0.049030544488711816, 0.030312084993359894), (0.049030544488711816, 0.030312084993359894), (0.049030544488711816, 0.030312084993359894), (0.049030544488711816, 0.030312084993359894), (0.049030544488711816, 0.030312084993359894), (0.04903386454183267, 0.030312084993359894), (0.04903386454183267, 0.030315405046480743), (0.04903386454183267, 0.030315405046480743), (0.04903386454183267, 0.030315405046480743), (0.04903386454183267, 0.030322045152722445), (0.04903386454183267, 0.030322045152722445), (0.04903386454183267, 0.030322045152722445), (0.04903386454183267, 0.030322045152722445), (0.04903718459495352, 0.030322045152722445), (0.04903718459495352, 0.030322045152722445), (0.04904382470119522, 0.030325365205843294), (0.04904382470119522, 0.030332005312084992), (0.04904382470119522, 0.030332005312084992), (0.04904382470119522, 0.030332005312084992), (0.04904382470119522, 0.030332005312084992), (0.04904714475431607, 0.030332005312084992), (0.04904714475431607, 0.030332005312084992), (0.04907038512616202, 0.030332005312084992), (0.04907038512616202, 0.030332005312084992), (0.04907038512616202, 0.030335325365205845), (0.04907038512616202, 0.030351925630810093), (0.04907038512616202, 0.030351925630810093), (0.04907038512616202, 0.030351925630810093), (0.04907038512616202, 0.030351925630810093), (0.04907038512616202, 0.030351925630810093), (0.04907038512616202, 0.030355245683930943), (0.04907038512616202, 0.030355245683930943), (0.04907038512616202, 0.030355245683930943), (0.04907038512616202, 0.030355245683930943), (0.04909030544488712, 0.030355245683930943), (0.04909030544488712, 0.030355245683930943), (0.04909030544488712, 0.030355245683930943), (0.04909030544488712, 0.030355245683930943), (0.04909030544488712, 0.030355245683930943), (0.04909030544488712, 0.030355245683930943), (0.04909030544488712, 0.030355245683930943), (0.04909030544488712, 0.03037184594953519), (0.04909030544488712, 0.03037184594953519), (0.04909694555112882, 0.030375166002656044), (0.04909694555112882, 0.030375166002656044), (0.04912018592297477, 0.030375166002656044), (0.04912018592297477, 0.030375166002656044), (0.04912018592297477, 0.030375166002656044), (0.04912018592297477, 0.030375166002656044), (0.04912018592297477, 0.030375166002656044), (0.04912018592297477, 0.030375166002656044), (0.04912018592297477, 0.030375166002656044), (0.04912018592297477, 0.030378486055776893), (0.04912018592297477, 0.030378486055776893), (0.04912018592297477, 0.030378486055776893), (0.04912018592297477, 0.030378486055776893), (0.04914342629482072, 0.030378486055776893), (0.04914342629482072, 0.030378486055776893), (0.04914342629482072, 0.030378486055776893), (0.04914342629482072, 0.030378486055776893), (0.04914342629482072, 0.030395086321381142)]\n",
      "[(0.05295515234251939, 0.0274709716617411), (0.05295515234251939, 0.0274709716617411), (0.05295702755816919, 0.0274709716617411), (0.05295702755816919, 0.0274709716617411), (0.05295702755816919, 0.0274709716617411), (0.05295702755816919, 0.0274709716617411), (0.05295702755816919, 0.0274709716617411)]\n"
     ]
    }
   ],
   "source": [
    "one_senti = find_senti(one_star)\n",
    "two_senti = find_senti(two_star)\n",
    "three_senti = find_senti(three_star)\n",
    "four_senti = find_senti(four_star)\n",
    "five_senti = find_senti(five_star)\n",
    "\n",
    "print(one_senti)\n",
    "print(two_senti)\n",
    "print(three_senti)\n",
    "print(four_senti)\n",
    "print(five_senti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
